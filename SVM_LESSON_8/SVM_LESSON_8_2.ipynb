{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7a97a0-53d0-414e-976c-ea33bc4b355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_sample(fn):\n",
    "    with open(fn, 'r') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def word_tokenize(content):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    return: list of tokens (str) e.g. ['>', 'Anyone', 'knows', 'how', 'much', 'it', 'costs', 'to', 'host', 'a']\n",
    "    '''\n",
    "\n",
    "    tokens = re.split(r'[ \\n]', content)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def lower_case(tokens):\n",
    "    tokens = [sub.lower() for sub in tokens]\n",
    "    tokens = np.array(tokens)\n",
    "    return tokens\n",
    "\n",
    "def normalize_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of tokens replaced with corresponding unified words\n",
    "    '''\n",
    "\n",
    "    # Function to replace numbers with \"number\"\n",
    "    def replace_numbers(token):\n",
    "        return re.sub(r'\\d+', 'number', token)\n",
    "\n",
    "    # Function to replace URLs with \"httpaddr\"\n",
    "    def replace_urls(token):\n",
    "        return re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'httpaddr', token)\n",
    "\n",
    "    # Function to replace emails with \"emailaddr\"\n",
    "    def replace_emails(token):\n",
    "        return re.sub(r'\\S+@\\S+', 'emailaddr', token)\n",
    "\n",
    "    # Function to replace $ with \"dollar\"\n",
    "    def replace_dollar(token):\n",
    "        return re.sub(r'\\$', 'dollar', token)\n",
    "\n",
    "    # Function to remove punctuation and non-alphanumeric characters\n",
    "    def remove_punctuation(token):\n",
    "        return re.sub(r'[^a-zA-Z0-9]', '', token)\n",
    "\n",
    "    # Apply the defined functions to each token\n",
    "    tokens = [replace_numbers(token) for token in tokens]\n",
    "    tokens = [replace_urls(token) for token in tokens]\n",
    "    tokens = [replace_emails(token) for token in tokens]\n",
    "    tokens = [replace_dollar(token) for token in tokens]\n",
    "    tokens = [remove_punctuation(token) for token in tokens]\n",
    "    \n",
    "    return np.array(tokens)\n",
    "\n",
    "def filter_short_tokens (tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of filtered tokens (str)\n",
    "    '''\n",
    "    original_tokens_len = len(tokens)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) < 1:\n",
    "            tokens = np.delete(tokens, np.where(tokens == token))\n",
    "   \n",
    "    print('Original len = {}\\nRemaining len = {}'.format(original_tokens_len, len(tokens)))    \n",
    "    return tokens\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of stemmed tokens e.g. array(['anyon', 'know', 'how', 'much', 'it', 'cost', 'to', 'host', 'a',\n",
    "       'web', 'portal', 'well', 'it', 'depend', 'on', 'how', 'mani']...\n",
    "    '''\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    # Apply the stemmer to each token\n",
    "    tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "   \n",
    "    return np.array(tokens)\n",
    "\n",
    "def get_vocabulary(fn):\n",
    "    '''\n",
    "    fn: str - full path to file \n",
    "    return: ndarray of str e.g. array(['aa', 'ab', 'abil', ..., 'zdnet', 'zero', 'zip'], dtype=object)\n",
    "    '''\n",
    "    vocab_list = pd.read_table(fn, header=None)\n",
    "    vocab = np.array(vocab_list)[:,1] # first columns is index, select only words column  \n",
    "    print ('len(vocab)= {:,}'.format(len(vocab)))\n",
    "    return vocab\n",
    "\n",
    "def represent_features(tokens,vocab):\n",
    "    '''\n",
    "    tokens: ndarry of str\n",
    "    tokens: ndarry of str\n",
    "    return: ndarry of binary values 1 if word from vocabulary is in mail 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    tokens_represented = list()\n",
    "    vocab = get_vocabulary('vocab.txt')\n",
    "\n",
    "    for word in vocab:\n",
    "        if word in tokens:\n",
    "            tokens_represented.append(1)\n",
    "        else:\n",
    "            tokens_represented.append(0)\n",
    "    tokens_represented = np.array(tokens_represented)   \n",
    "\n",
    "    print ('{} word(s) from vocab are in the tokens.'.format(np.sum(tokens_represented)))\n",
    "\n",
    "    return tokens_represented\n",
    "\n",
    "def preprocess (content,vocab):\n",
    "    '''\n",
    "    content: str - body of mail \n",
    "    vocab: ndarray of str - list of considered words \n",
    "    '''\n",
    "    # tokenize content    \n",
    "    tokens = word_tokenize(content)\n",
    "    \n",
    "    # make lower case\n",
    "    tokens = lower_case(tokens)\n",
    "\n",
    "    # normalize tokens\n",
    "    tokens = normalize_tokens(tokens)\n",
    "\n",
    "    # remove zero words\n",
    "    tokens = filter_short_tokens(tokens)\n",
    "    \n",
    "    # stem words\n",
    "    tokens = stem_tokens(tokens)\n",
    "    \n",
    "    # convert to binary array of features  \n",
    "    tokens_represented = represent_features(tokens, vocab)    \n",
    "    \n",
    "    return tokens_represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "016ad7cf-e416-47d6-a790-8f56ce5794ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= {} (4000, 1899)\n",
      "y_train.shape= {} (4000,)\n",
      "X_test.shape= {} (1000, 1899)\n",
      "y_test.shape= {} (1000,)\n",
      "Sample with index  =0: \n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "fn = 'spamTrain.mat'\n",
    "\n",
    "mat= loadmat(fn)\n",
    "X_train= mat['X']\n",
    "y_train= mat['y'].ravel()\n",
    "\n",
    "print ('X_train.shape= {}',X_train.shape)\n",
    "print ('y_train.shape= {}',y_train.shape)\n",
    "\n",
    "fn = 'spamTest.mat'\n",
    "mat= loadmat(fn)\n",
    "X_test = mat['Xtest']\n",
    "y_test = mat['ytest'].ravel() \n",
    "\n",
    "print ('X_test.shape= {}',X_test.shape)\n",
    "print ('y_test.shape= {}',y_test.shape)\n",
    "index = 0 \n",
    "print ('Sample with index  ={}: \\n{}'.format(index, X_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff00bd92-a70f-4e95-91d7-374fa4bec28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train = 0.99975\n",
      "Score test = 0.992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "C = .1\n",
    "clf= LinearSVC(C=C)\n",
    "clf.fit(X_train,y_train)\n",
    "print ('Score train = {}'.format(clf.score(X_train,y_train)))\n",
    "print ('Score test = {}'.format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d1758e-d0d1-487f-93bf-1f080fa81882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab)= 1,899\n",
      "['our', 'remov', 'click', 'basenumb', 'guarante', 'visit', 'bodi', 'will', 'numberb', 'price', 'dollar', 'nbsp', 'below', 'lo', 'most', 'send', 'dollarnumb', 'credit', 'wi', 'hour']\n"
     ]
    }
   ],
   "source": [
    "coefs = clf.coef_[0]\n",
    "sorted = np.sort(coefs)[::-1]\n",
    "top_20 = sorted[:20]\n",
    "top_20_indexes = np.full(20, np.nan)\n",
    "for i in range(20):\n",
    "    top_20_indexes[i] = np.where(coefs == top_20[i])[0][0]\n",
    "\n",
    "vocab = get_vocabulary('vocab.txt')\n",
    "# Get the corresponding words from the vocabulary\n",
    "top_spam_contributors = [vocab[int(index)] for index in top_20_indexes]\n",
    "print(top_spam_contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1e4f74-2f2c-43f8-b8b6-bdb5ff9c2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original len = 67\n",
      "Remaining len = 61\n",
      "len(vocab)= 1,899\n",
      "44 word(s) from vocab are in the tokens.\n",
      "[0]\n",
      "emailSample1.txt is Not Spam\n",
      "\n",
      "Original len = 247\n",
      "Remaining len = 222\n",
      "len(vocab)= 1,899\n",
      "122 word(s) from vocab are in the tokens.\n",
      "[0]\n",
      "emailSample2.txt is Not Spam\n",
      "\n",
      "Original len = 141\n",
      "Remaining len = 97\n",
      "len(vocab)= 1,899\n",
      "46 word(s) from vocab are in the tokens.\n",
      "[1]\n",
      "spamSample1.txt is Spam\n",
      "\n",
      "Original len = 39\n",
      "Remaining len = 31\n",
      "len(vocab)= 1,899\n",
      "18 word(s) from vocab are in the tokens.\n",
      "[1]\n",
      "spamSample2.txt is Spam\n",
      "\n",
      "Latter sample:\n",
      "==================================================\n",
      "Best Buy Viagra Generic Online\n",
      "\n",
      "Viagra 100mg x 60 Pills $125, Free Pills & Reorder Discount, Top Selling 100% Quality & Satisfaction guaranteed!\n",
      "\n",
      "We accept VISA, Master & E-Check Payments, 90000+ Satisfied Customers!\n",
      "http://medphysitcstech.ru\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for sfn in [ 'emailSample1.txt', 'emailSample2.txt', 'spamSample1.txt', 'spamSample2.txt']:\n",
    "    fn = sfn\n",
    "    content = get_sample(fn)\n",
    "    \n",
    "    # YOUR_CODE.  Preprocess the sample and get prediction 0 or 1 (1 is spam)\n",
    "    # START_CODE \n",
    "    preprocessed = preprocess(content, vocab)\n",
    "    preprocessed = preprocessed.reshape(1, -1)\n",
    "    prediction = clf.predict(preprocessed)\n",
    "    # END_CODE    \n",
    "    print(prediction)\n",
    "    print ('{} is {}\\n'.format(sfn, ('Not Spam','Spam')[prediction[0]]))\n",
    "\n",
    "print ('Latter sample:\\n{1}\\n{0}\\n{1}'.format(content, '='*50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
